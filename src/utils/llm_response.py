from typing import Any
from schemas import UserRequestMetadata, QueryResponse
from utils.clients import openai_client
from config import settings
from utils.custom_logging import get_logger
from langfuse.decorators import observe

logger = get_logger()


@observe()
def llm_metadata_extracted(
    user_input: str,
    client: Any = openai_client,
    model_name: str = settings.LLM_MODEL_NAME,
) -> UserRequestMetadata:
    """
    Extracts metadata fields from a user's input using an LLM.

    This function sends the user input to a language model, requesting it to identify and extract metadata fields such as 'date', 'speaker_name',
    'meeting_title', or 'position' from the input. If no metadata is found, the function will return an empty list or dictionary.

    Args:
        user_input (str): The input provided by the user, containing potential metadata.
        client (Any, optional): The client to interact with the LLM (default is `openai_client`).
        model_name (str, optional): The name of the model to use (default is set in `settings.LLM_MODEL_NAME`).

    Returns:
        UserRequestMetadata: A parsed response containing the extracted metadata fields, or an empty result if no metadata is found.
    """
    messages = [
        {
            "role": "system",
            "content": """
            Identify the metadata fields in the user's request, such as 'date', 'speaker_name', 'meeting_title', or 'position'.
            Always extract these metadata types when they appear in the request. If no metadata is provided, ensure an empty list/dictionary.
        """,
        },
        {"role": "user", "content": user_input},
    ]

    # Call the LLM to get a response
    logger.info("Calling the LLM...")
    completion = client.beta.chat.completions.parse(
        model=model_name,
        messages=messages,
        response_format=UserRequestMetadata,
    )

    # Parse the result
    result = completion.choices[0].message.parsed

    return result


def generate_response(
    user_input: str,
    conversation: str,
    client: Any = openai_client,
    model_name: str = settings.LLM_MODEL_NAME,
) -> QueryResponse:
    """
    Generates a response based on the user's input and conversation history, utilizing the LLM.

    This function takes the user's current input along with the historical conversation context and
    relevant metadata (such as extracted date, speaker name, etc.) to generate a contextually appropriate response.
    It sends this information to the LLM and processes the model’s output to return a response that fits within the ongoing conversation.

    Args:
        user_input (str): The current message or query from the user.
        conversation (str): A string containing the conversation history to provide context to the LLM.
        client (Any, optional): The LLM client used to make the request (default is `openai_client`).
        model_name (str, optional): The model to be used for generating the response (default is set in `settings.LLM_MODEL_NAME`).

    Returns:
        QueryResponse: A Pydantic model containing the response generated by the LLM along with a confidence score indicating
        the quality or certainty of the response.
    """
    # Prepare messages with conversation history and the user's input
    messages = [
        {
            "role": "system",
            "content": (
                "You are an assistant tasked with providing insights solely based on the meeting transcript data provided below. "
                "Each entry includes the speaker and their statements during the meeting. "
                "Please base your response entirely on this conversation history, without incorporating any external information. "
                "Respond accurately to the user’s query by referencing only the statements made by the speakers during the meeting."
            ),
        },
        {
            "role": "system",
            "content": f"Conversation:\n{conversation}",
        },  # Conversation history
        {"role": "user", "content": f"User question: {user_input}"},  # User's question
    ]

    # Call the LLM to get a response
    logger.info("Calling the LLM...")
    completion = client.beta.chat.completions.parse(
        model=model_name,
        messages=messages,
        response_format=QueryResponse,
    )

    # Parse the result
    result = completion.choices[0].message.parsed

    return result
